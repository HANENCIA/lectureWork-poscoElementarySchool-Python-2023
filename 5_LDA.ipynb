{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbae48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hanencia\\anaconda3\\envs\\common\\lib\\site-packages (1.25.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d990d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def warning_handler(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "warnings.warn = warning_handler\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "# from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self,\n",
    "                 raw_excel_path,\n",
    "                 raw_sheet_name=\"sheet\",\n",
    "                 keyword_col_name=\"keyword\",\n",
    "                 date_col_name=\"date\",\n",
    "                 n_gram=1,\n",
    "                 max_relative_frequency=1,\n",
    "                 min_absolute_frequency=0,\n",
    "                 max_features=100000):\n",
    "        self._raw_excel_path = raw_excel_path\n",
    "        self.date_col_name = date_col_name\n",
    "        self._n_gram = n_gram\n",
    "        self._max_relative_frequency = max_relative_frequency\n",
    "        self._min_absolute_frequency = min_absolute_frequency\n",
    "        self._max_features = max_features\n",
    "        self.data_frame = pd.ExcelFile(raw_excel_path).parse(raw_sheet_name)\n",
    "        self.data_frame[date_col_name] = self.data_frame[date_col_name].astype(str).apply(\n",
    "            lambda x: datetime.strptime(f\"{x[:4]}-{x[4:6]}-{x[6:]}\", \"%Y-%m-%d\").year)\n",
    "        self.data_frame.fillna(' ')\n",
    "        self.size = self.data_frame.count(0)[0]\n",
    "\n",
    "        stop_words = []\n",
    "        # stop_words = stopwords.words('english')\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, self._n_gram),\n",
    "                                     max_df=self._max_relative_frequency,\n",
    "                                     min_df=self._min_absolute_frequency,\n",
    "                                     max_features=self._max_features,\n",
    "                                     stop_words=stop_words)\n",
    "        self.sklearn_vector_space = vectorizer.fit_transform(self.data_frame[keyword_col_name].tolist())\n",
    "        self.gensim_vector_space = None\n",
    "#         vocab = vectorizer.get_feature_names()\n",
    "        vocab = vectorizer.get_feature_names_out()\n",
    "        self.vocabulary = dict([(i, s) for i, s in enumerate(vocab)])\n",
    "\n",
    "    def date(self, doc_id, date_col_name):\n",
    "        return self.data_frame.iloc[doc_id][date_col_name]\n",
    "\n",
    "    def word_for_id(self, word_id):\n",
    "        return self.vocabulary.get(word_id)\n",
    "\n",
    "    def doc_ids(self, date, date_col_name):\n",
    "        return self.data_frame[self.data_frame[date_col_name] == date].index.tolist()\n",
    "\n",
    "\n",
    "class TopicModel(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.date_col_name = corpus.date_col_name\n",
    "        self.document_topic_matrix = None\n",
    "        self.topic_word_matrix = None\n",
    "        self.nb_topics = None\n",
    "\n",
    "    def infer_topics(self, num_topics=None, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def get_topics(self, num_words=None):\n",
    "        frequency = self.topics_frequency()\n",
    "        topic_lst = []\n",
    "\n",
    "        for topic_id in range(self.nb_topics):\n",
    "            word_list = []\n",
    "            for weighted_word in self.top_words(topic_id, num_words):\n",
    "                word_list.append(weighted_word[0])\n",
    "            topic_lst.append((topic_id, frequency[topic_id], \", \".join(word_list)))\n",
    "\n",
    "        return topic_lst\n",
    "\n",
    "    def topic_frequency(self, topic, date=None):\n",
    "        return self.topics_frequency(date=date)[topic]\n",
    "\n",
    "    def topics_frequency(self, date=None):\n",
    "        frequency = np.zeros(self.nb_topics)\n",
    "        if date is None:\n",
    "            ids = range(self.corpus.size)\n",
    "        else:\n",
    "            ids = self.corpus.doc_ids(date, self.date_col_name)\n",
    "\n",
    "        for i in ids:\n",
    "            topic = self.most_likely_topic_for_document(i)\n",
    "            frequency[topic] += 1.0 / len(ids)\n",
    "        return frequency\n",
    "\n",
    "    def most_likely_topic_for_document(self, doc_id):\n",
    "        weights = list(self.topic_distribution_for_document(doc_id))\n",
    "        return weights.index(max(weights))\n",
    "\n",
    "    def topic_distribution_for_document(self, doc_id):\n",
    "        vector = self.document_topic_matrix[doc_id].toarray()\n",
    "        return vector[0]\n",
    "\n",
    "    def top_words(self, topic_id, num_words):\n",
    "        vector = self.topic_word_matrix[topic_id]\n",
    "        cx = vector.tocoo()\n",
    "        weighted_words = [()] * len(self.corpus.vocabulary)\n",
    "        for row, word_id, weight in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "            weighted_words[word_id] = (self.corpus.word_for_id(word_id), weight)\n",
    "        weighted_words.sort(key=lambda x: x[1], reverse=True)\n",
    "        return weighted_words[:num_words]\n",
    "\n",
    "\n",
    "class LatentDirichletAllocation(TopicModel):\n",
    "    def infer_topics(self, num_topics=None, max_iter=100, **kwargs):\n",
    "        self.nb_topics = num_topics\n",
    "        # lda_model = NMF(n_components=num_topics)\n",
    "        lda_model = LDA(n_components=num_topics, learning_method='batch', max_iter=max_iter)\n",
    "        topic_document = lda_model.fit_transform(self.corpus.sklearn_vector_space)\n",
    "\n",
    "        self.topic_word_matrix = []\n",
    "        self.document_topic_matrix = []\n",
    "        vocabulary_size = len(self.corpus.vocabulary)\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            for i in range(vocabulary_size):\n",
    "                row.append(topic_idx)\n",
    "                col.append(i)\n",
    "                data.append(topic[i])\n",
    "        self.topic_word_matrix = coo_matrix((data, (row, col)),\n",
    "                                            shape=(self.nb_topics, len(self.corpus.vocabulary))).tocsr()\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "        doc_count = 0\n",
    "        for doc in topic_document:\n",
    "            topic_count = 0\n",
    "            for topic_weight in doc:\n",
    "                row.append(doc_count)\n",
    "                col.append(topic_count)\n",
    "                data.append(topic_weight)\n",
    "                topic_count += 1\n",
    "            doc_count += 1\n",
    "        self.document_topic_matrix = coo_matrix((data, (row, col)), shape=(self.corpus.size, self.nb_topics)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d559d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 빅카인즈 신문기사 파일\n",
    "    raw_excel_path = \"./data/NewsResult_20230810-20230910.xlsx\"\n",
    "    raw_sheet_name = \"sheet\"\n",
    "\n",
    "    keyword_col_name = \"키워드\"\n",
    "    date_col_name = \"일자\"\n",
    "\n",
    "    tfidf_max_features = 100000\n",
    "    tfidf_max_tf = 1.0\n",
    "    tfidf_min_tf = 1\n",
    "    tfidf_ngram = 2\n",
    "\n",
    "    # 토픽 갯수\n",
    "    n_topics = 10\n",
    "    # 반복 횟수\n",
    "    n_iter = 10\n",
    "\n",
    "    # 토픽 당 단어 갯수\n",
    "    top_n_words = 30\n",
    "    # 분석 시작 년도\n",
    "    start_year = 2023\n",
    "    # 분석 끝 년도\n",
    "    end_year = 2023\n",
    "\n",
    "    # 결과 파일\n",
    "    result_topic_words_csv_path = \"./topic_words.csv\"\n",
    "    result_topic_ratio_year_csv_path = \"./topic_ratio_year.csv\"\n",
    "\n",
    "    os.makedirs(\"/\".join(result_topic_words_csv_path.split(\"/\")[:-1]), exist_ok=True)\n",
    "    os.makedirs(\"/\".join(result_topic_ratio_year_csv_path.split(\"/\")[:-1]), exist_ok=True)\n",
    "\n",
    "    corpus = Corpus(raw_excel_path, raw_sheet_name, keyword_col_name, date_col_name, tfidf_ngram, tfidf_max_tf,\n",
    "                    tfidf_min_tf, tfidf_max_features)\n",
    "\n",
    "    topic_model = LatentDirichletAllocation(corpus=corpus)\n",
    "\n",
    "    topic_model.infer_topics(num_topics=n_topics, max_iter=n_iter)\n",
    "\n",
    "    topic_words_lst = topic_model.get_topics(num_words=top_n_words)\n",
    "    topic_words_df = pd.DataFrame(topic_words_lst, columns=[\"Topic No\", \"Frequency\", \"Words\"])\n",
    "    topic_words_df.to_csv(result_topic_words_csv_path, sep=\",\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "    topic_ratio_yr_lst = []\n",
    "    for topic_id in range(topic_model.nb_topics):\n",
    "        topic_ratio_yr_tmp = []\n",
    "        for i in range(start_year, end_year + 1):\n",
    "            topic_ratio_yr_tmp.append(topic_model.topic_frequency(topic_id, date=i))\n",
    "        topic_ratio_yr_lst.append(topic_ratio_yr_tmp)\n",
    "\n",
    "    year_topic_ratio_col_names = []\n",
    "    for e in range(start_year, end_year + 1):\n",
    "        year_topic_ratio_col_names.append(e)\n",
    "\n",
    "    topic_ratio_yr_df = pd.DataFrame(topic_ratio_yr_lst, columns=year_topic_ratio_col_names)\n",
    "    topic_ratio_yr_df.to_csv(result_topic_ratio_year_csv_path, sep=\",\", encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904063a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
